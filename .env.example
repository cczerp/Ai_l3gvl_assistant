# ========================================
# LEGAL-AI ASSISTANT - ENVIRONMENT CONFIGURATION
# ========================================
# Copy this file to .env and fill in your actual values
# NEVER commit .env to git (it's in .gitignore)

# ========================================
# PAID CLOUD AI MODELS (OPTIONAL)
# ========================================
# OpenAI API (GPT-4) - https://platform.openai.com/api-keys
# Cost: ~$0.01-0.05 per query
# Leave blank if using only free alternatives
OPENAI_API_KEY=

# Anthropic API (Claude 3 Opus) - https://console.anthropic.com/
# Cost: Pay-per-use
# Leave blank if using only free alternatives
ANTHROPIC_API_KEY=

# ========================================
# FREE AI ALTERNATIVES (RECOMMENDED FOR TESTING)
# ========================================
# Option 1: HuggingFace Inference API (FREE tier: 30,000 requests/month)
# Sign up: https://huggingface.co/settings/tokens
# Models available: Llama-2, Mistral, Mixtral, Zephyr, etc.
HUGGINGFACE_API_KEY=

# Option 2: Groq API (FREE tier: Fast inference for Llama-3, Mixtral)
# Sign up: https://console.groq.com/
# FREE tier: 14,400 requests/day (10 req/min)
GROQ_API_KEY=

# Option 3: Use local models only (no API key needed)
# Set USE_LOCAL_MODELS_ONLY=true to disable all cloud APIs
USE_LOCAL_MODELS_ONLY=false

# ========================================
# DATABASE CONFIGURATION
# ========================================
# Supabase (PostgreSQL + pgvector) - RECOMMENDED
# Sign up: https://supabase.com/
# Free tier: 500 MB storage, 2 GB bandwidth/month
# Pro tier: $25/month, 100 GB storage
SUPABASE_URL=
SUPABASE_KEY=

# Alternative: Direct PostgreSQL connection (if not using Supabase)
# Example: postgresql://username:password@localhost:5432/legal_ai
DATABASE_URL=

# ========================================
# VECTOR STORE CONFIGURATION
# ========================================
# Choose one vector store option below:

# Option 1: FAISS (FREE, local, file-based) - DEFAULT
# No setup needed - automatically created in data/vector_store/
VECTOR_STORE_TYPE=faiss

# Option 2: ChromaDB (FREE, local) - Alternative to FAISS
# No API key needed - runs locally
# VECTOR_STORE_TYPE=chromadb

# Option 3: Qdrant (FREE tier available, cloud or local)
# Sign up: https://cloud.qdrant.io/
# Free tier: 1GB storage
# VECTOR_STORE_TYPE=qdrant
QDRANT_API_KEY=
QDRANT_URL=

# Option 4: Pinecone (Paid service)
# Sign up: https://www.pinecone.io/
# VECTOR_STORE_TYPE=pinecone
PINECONE_API_KEY=
PINECONE_ENVIRONMENT=

# ========================================
# EMBEDDINGS CONFIGURATION
# ========================================
# Choose embedding model:

# Option 1: OpenAI Embeddings (Requires OPENAI_API_KEY)
# Model: text-embedding-ada-002, Dimension: 1536
# Cost: $0.0001 per 1K tokens
EMBEDDING_MODEL=openai

# Option 2: FREE Local Embeddings (No API key needed)
# Model: sentence-transformers/all-MiniLM-L6-v2
# Dimension: 384, Speed: Fast, Quality: Good
# EMBEDDING_MODEL=local

# Option 3: FREE HuggingFace Embeddings (Requires HUGGINGFACE_API_KEY)
# Model: sentence-transformers/all-mpnet-base-v2
# Dimension: 768, Quality: Better than MiniLM
# EMBEDDING_MODEL=huggingface

# ========================================
# LOCAL MODEL CONFIGURATION
# ========================================
# Paths to downloaded model weights (optional)
# Download from HuggingFace - see docs/setup.md for instructions

# Llama3 8B Instruct (~16 GB) - meta-llama/Meta-Llama-3-8B-Instruct
LLAMA3_MODEL_PATH=models/llama3-8b-instruct

# Mixtral 8x7B Instruct (~47 GB) - mistralai/Mixtral-8x7B-Instruct-v0.1
MIXTRAL_MODEL_PATH=models/mixtral-8x7b-instruct

# Model quantization (4bit, 8bit, or none)
# 4bit: Lower memory (~4-6 GB), faster, slight quality loss
# 8bit: Medium memory (~8-10 GB), balanced
# none: Full precision (~16+ GB), best quality
MODEL_QUANTIZATION=4bit

# Enable GPU acceleration (true/false)
# Requires CUDA-capable GPU
USE_GPU=false

# ========================================
# DATA STORAGE PATHS
# ========================================
# Legal data storage directories
STATE_LAWS_PATH=data/state_laws
FEDERAL_LAWS_PATH=data/federal_laws
CASES_PATH=data/cases
LEGAL_DICTIONARIES_PATH=data/legal_dictionaries

# Vector store and cache directories
VECTOR_STORE_PATH=data/vector_store
EMBEDDINGS_CACHE_PATH=data/embeddings
DISK_CACHE_PATH=data/cache

# ========================================
# API CONFIGURATION
# ========================================
# API server settings
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# JWT Secret Key for API authentication
# Generate with: openssl rand -hex 32
API_SECRET_KEY=

# API Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS_PER_MINUTE=60
RATE_LIMIT_BURST_SIZE=10

# CORS Settings (comma-separated origins)
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080

# ========================================
# MODEL ROUTING STRATEGY
# ========================================
# How to choose between models for queries

# cost_optimized: Use cheapest model first, fallback to expensive
# performance: Use best model first, fallback to cheaper
# hybrid: Use cheap for simple queries, expensive for complex
# local_only: Use only local models (no API calls)
MODEL_ROUTING_STRATEGY=cost_optimized

# Enable automatic fallback if primary model fails
MODEL_FALLBACK_ENABLED=true

# ========================================
# RETRIEVAL-AUGMENTED GENERATION (RAG)
# ========================================
# Number of similar documents to retrieve
RAG_TOP_K=5

# Minimum similarity score (0.0 to 1.0)
RAG_SIMILARITY_THRESHOLD=0.7

# Maximum context length to send to LLM
RAG_MAX_CONTEXT_LENGTH=4000

# Enable reranking of retrieved documents
RAG_RERANKING_ENABLED=true

# ========================================
# LOGGING & MONITORING
# ========================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: json or text
LOG_FORMAT=json

# Log file path (leave empty for stdout only)
LOG_FILE_PATH=logs/app.log

# Enable performance metrics
ENABLE_METRICS=true

# ========================================
# CACHING CONFIGURATION
# ========================================
# Enable response caching for repeated queries
CACHE_ENABLED=true

# Cache TTL in seconds (3600 = 1 hour)
CACHE_TTL=3600

# Cache backend: memory, disk, or redis
CACHE_BACKEND=disk

# Redis configuration (if using redis cache)
REDIS_URL=redis://localhost:6379/0

# ========================================
# WEB SCRAPING CONFIGURATION
# ========================================
# User agent for web scraping
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36

# Request timeout in seconds
REQUEST_TIMEOUT=30

# Max retries for failed requests
MAX_RETRIES=3

# Delay between requests (seconds) to be polite
SCRAPING_DELAY=1.0

# Enable JavaScript rendering with Playwright (slower but works for dynamic sites)
ENABLE_PLAYWRIGHT=false
# Optional: persisted Playwright storage state (e.g., cf_clearance cookie)
PLAYWRIGHT_STORAGE_STATE=

# ========================================
# FEATURE FLAGS
# ========================================
# Enable specific features (true/false)
ENABLE_STATE_LAWS=true
ENABLE_FEDERAL_LAWS=true
ENABLE_CASE_LAW=true
ENABLE_LEGAL_DICTIONARY=true
ENABLE_PRECEDENT_TRACKING=true
ENABLE_CITATION_ANALYSIS=true

# ========================================
# DEVELOPMENT & TESTING
# ========================================
# Environment: development, staging, production
ENVIRONMENT=development

# Enable debug mode (more verbose logging)
DEBUG=false

# Enable auto-reload for development
AUTO_RELOAD=true

# Test mode (uses smaller datasets, faster)
TEST_MODE=false
