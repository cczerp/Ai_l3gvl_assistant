"""
Query endpoints for legal question answering.
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict, Any

from src.router import ModelRouter, QueryType
from src.rag import RetrievalService
from src.models import OpenAIModel, LlamaModel

router = APIRouter()


class QueryRequest(BaseModel):
    """Request model for legal queries."""
    query: str
    jurisdiction: Optional[str] = None
    use_rag: bool = True
    prefer_local: bool = False
    max_results: int = 5


class QueryResponse(BaseModel):
    """Response model for legal queries."""
    answer: str
    sources: List[Dict[str, Any]]
    model_used: str
    confidence: Optional[float] = None


@router.post("/query", response_model=QueryResponse)
async def query_legal_question(request: QueryRequest):
    """
    Submit a legal query and get an answer with citations.
    
    Args:
        request: Query request containing question and parameters
        
    Returns:
        Answer with sources and metadata
    """
    try:
        # Initialize services
        router_service = ModelRouter()
        
        # Route to appropriate model
        model_info = router_service.route_query(
            query=request.query,
            query_type=QueryType.LEGAL_ANALYSIS,
            prefer_local=request.prefer_local
        )
        
        # Retrieve context if RAG enabled
        context = None
        sources = []
        if request.use_rag:
            retrieval = RetrievalService()
            results = retrieval.retrieve(
                query=request.query,
                top_k=request.max_results,
                filter_criteria={"jurisdiction": request.jurisdiction} if request.jurisdiction else None
            )
            sources = results
            context = retrieval.get_context_window(request.query)
        
        # Generate answer (stub - would call actual model)
        answer = f"Based on the legal analysis of: {request.query}\n\n"
        answer += f"[Answer generated by {model_info['model_name']} would appear here]"
        
        return QueryResponse(
            answer=answer,
            sources=sources,
            model_used=model_info['model_name'],
            confidence=0.85
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/query/history")
async def get_query_history(limit: int = 10):
    """
    Get query history.
    
    Args:
        limit: Maximum number of queries to return
        
    Returns:
        List of recent queries
    """
    # Stub implementation
    return {
        "queries": [],
        "total": 0,
        "limit": limit
    }
